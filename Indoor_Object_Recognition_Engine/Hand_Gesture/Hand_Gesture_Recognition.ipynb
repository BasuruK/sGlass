{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nipuni\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam,adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative', 'Positive']\n",
      "Loaded the images of dataset-Negative\n",
      "\n",
      "['Negative', 'Positive']\n",
      "Loaded the images of dataset-Positive\n",
      "\n",
      "Image Data (234, 49152)\n",
      "Image Data Scaled [[-0.06999187 -0.06604903 -0.03929729 ..., -0.16223969 -0.11872931\n",
      "  -0.07215629]\n",
      " [-0.35696729 -0.39779527 -0.38733662 ...,  0.39891226  0.28597075\n",
      "   0.34767629]\n",
      " [-0.09049011 -0.02702006 -0.05761515 ..., -0.11907416 -0.24013933\n",
      "  -0.4189745 ]\n",
      " ..., \n",
      " [-0.60294623 -0.78808498 -0.73537595 ...,  2.79459946  2.91652113\n",
      "   2.84841817]\n",
      " [-0.02899538 -0.18313594 -0.13088659 ..., -1.06871593 -1.04953945\n",
      "  -0.96658222]\n",
      " [-0.50045501 -0.45633873 -0.3507009  ...,  0.33416396  0.42761577\n",
      "   0.45719783]]\n",
      "Image Data Scaled RGB (234, 128, 128, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nipuni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(128, 128,..., padding=\"same\")`\n",
      "C:\\Users\\Nipuni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:99: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n",
      "C:\\Users\\Nipuni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:102: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), data_format=\"channels_first\")`\n",
      "C:\\Users\\Nipuni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:105: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 126, 126, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 126, 126, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 126, 63, 16)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 126, 63, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 61, 14)        72640     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 61, 14)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 30, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64, 30, 7)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 13440)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               6881792   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 6,965,602.0\n",
      "Trainable params: 6,965,602.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Test Loss: 5.7956829071\n",
      "Test accuracy: 0.638297855854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path\n",
    "from keras.models import load_model\n",
    "\n",
    "USE_SKLEARN_PREPROCESSING=False\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def image_to_feature_vector(image,size=(128,128)):\n",
    "    #resize the image to a fixed size , then flttern the image into a list of raw pixels intensities\n",
    "    return cv2.resize(image,size).flatten()\n",
    "\n",
    "if USE_SKLEARN_PREPROCESSING:\n",
    "    img_data=img_data_scaled\n",
    "    \n",
    "PATH = os.getcwd()\n",
    "# Define data path\n",
    "data_path = PATH + '\\Original\\DATA SET'\n",
    "data_dir_list = os.listdir(data_path)\n",
    "\n",
    "img_rows=128\n",
    "img_colms=128\n",
    "num_channels=3\n",
    "num_epoch=6\n",
    "\n",
    "img_data_list=[]\n",
    "for dataset in data_dir_list:\n",
    "    print(data_dir_list)\n",
    "    img_list=os.listdir(data_path+'/'+dataset)\n",
    "    print('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "    for img in img_list:\n",
    "        input_img=cv2.imread(data_path+'/'+dataset+'/'+img)\n",
    "            \n",
    "        input_img_flatten=image_to_feature_vector(input_img,(128,128))\n",
    "            \n",
    "        img_data_list.append(input_img_flatten)\n",
    "\n",
    "img_data=np.array(img_data_list)\n",
    "img_data=img_data.astype('float')\n",
    "print('Image Data',img_data.shape)\n",
    "\n",
    "if num_channels==1:\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data=np.expand_dims(img_data,axis=1)\n",
    "        print('Image Data BnW',img_data.shape)\n",
    "    else:\n",
    "        img_data=np.expand_dims(img_data,axis=4)\n",
    "        print('Image Data BnW',img_data.shape)\n",
    "\n",
    "else:\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data=np.rollaxis(img_data,3,1)\n",
    "        print('Image Data RGB',img_data.shape)\n",
    "\n",
    "image_data_scaled=preprocessing.scale(img_data)\n",
    "print(\"Image Data Scaled\" , image_data_scaled)\n",
    "\n",
    "if K.image_dim_ordering()=='th':\n",
    "    image_data_scaled=image_data_scaled.reshape(img_data.shape[0],num_channels,img_rows,img_colms)\n",
    "    print('Image Data Scaled BnW',image_data_scaled.shape)\n",
    "else:\n",
    "    image_data_scaled=image_data_scaled.reshape(img_data.shape[0],img_rows,img_colms,num_channels)\n",
    "    print('Image Data Scaled RGB',image_data_scaled.shape)\n",
    "\n",
    "#Define classes\n",
    "\n",
    "num_classes=2\n",
    "\n",
    "num_samples=img_data.shape[0]\n",
    "labels=np.ones((num_samples,),dtype='int64')\n",
    "\n",
    "labels[0:113]=0\n",
    "labels[113:]=1\n",
    "\n",
    "names=['negative','positive']\n",
    "\n",
    "#convert class labels to on-hot encoding\n",
    "Y=np_utils.to_categorical(labels,num_classes)\n",
    "\n",
    "x,y=shuffle(img_data,Y,random_state=2)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=4)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_colms, img_rows, -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_colms, img_rows, -1)\n",
    "\n",
    "input_shape=(img_colms, img_rows, 1)\n",
    "\n",
    "# Defining the model\n",
    "input_shape=img_data[0].shape\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3),border_mode='same',input_shape=(128,128,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(32,( 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),dim_ordering='th'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3),dim_ordering='th'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),dim_ordering='th'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512)) #no of hidden layers\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "# Viewing model_configuration\n",
    "\n",
    "model.summary()\n",
    "model.get_config()\n",
    "model.layers[0].get_config()\n",
    "model.layers[0].input_shape\n",
    "model.layers[0].output_shape\n",
    "model.layers[0].get_weights()\n",
    "np.shape(model.layers[0].get_weights()[0])\n",
    "model.layers[0].trainable\n",
    "\n",
    "# Training\n",
    "\n",
    "if os.path.isfile('Hand_Gesture_Recognition.h5') == False:\n",
    "    hist = model.fit(X_train, y_train, batch_size=150, epochs=num_epoch, verbose=1, validation_data=(X_test, y_test))\n",
    "    model.save('Hand_Gesture_Recognition.h5')\n",
    "else:\n",
    "    hist=load_model('Hand_Gesture_Recognition.h5')\n",
    "\n",
    "# Evaluating the model\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size=150 , verbose=0) #batch_size=16\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "test_image = X_test[0:1]\n",
    "\n",
    "#print(model.predict(test_image))\n",
    "#print(model.predict_classes(test_image))\n",
    "y_test[0:1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing a new image\n",
    "def Get_Reult_Of_HG(filepath):\n",
    "\n",
    "    test_image = cv2.imread(filepath)\n",
    "    #test_image=cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
    "    test_image=cv2.resize(test_image,(128,128))\n",
    "    test_image = np.array(test_image)\n",
    "    test_image = test_image.astype('float32')\n",
    "    test_image /= 255\n",
    "    print (test_image.shape)\n",
    "\n",
    "    if num_channels==1:\n",
    "        if K.image_dim_ordering()=='th':\n",
    "            test_image= np.expand_dims(test_image, axis=1)\n",
    "            print (test_image.shape)\n",
    "\n",
    "        else:\n",
    "            test_image= np.expand_dims(test_image, axis=4) \n",
    "            print (test_image.shape)\n",
    "\n",
    "    else:\n",
    "        if K.image_dim_ordering()=='th':\n",
    "            test_image=np.rollaxis(test_image,3,1)\n",
    "            print (test_image.shape)\n",
    "        else:\n",
    "            test_image= np.expand_dims(test_image, axis=0)\n",
    "            print (test_image.shape)\n",
    "\n",
    "    # Predicting the test image\n",
    "    print(model.predict(test_image))\n",
    "    predict_class=model.predict_classes(test_image)\n",
    "    print(predict_class)\n",
    "\n",
    "    result=''\n",
    "\n",
    "    if predict_class==[1]:\n",
    "        print('Result :',predict_class,'=> Positive Hand Gesture')\n",
    "        result='Positive Hand Gesture'\n",
    "\n",
    "    elif predict_class==[0]:\n",
    "        print('Result :',predict_class,'=> Negative Hand Gesture')\n",
    "        result='Negative Hand Gesture'\n",
    "        \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def capture_object():\n",
    "\n",
    "    cam=cv2.VideoCapture(0)\n",
    "\n",
    "    cv2.namedWindow(\"test\")\n",
    "\n",
    "    img_counter=0\n",
    "\n",
    "    while True:\n",
    "        ret, frame=cam.read()\n",
    "        cv2.imshow(\"test\",frame)\n",
    "        if not ret:\n",
    "            break\n",
    "        k=cv2.waitKey(1)\n",
    "\n",
    "        if k%256 == 27:\n",
    "            #ESC pressed\n",
    "            print(\"Escape hit, closing...\")\n",
    "            break\n",
    "        elif k%256 == 32:\n",
    "            #SPACE pressed\n",
    "            img_name=\"opencv_frame_{}.png\".format(img_counter)\n",
    "\n",
    "            print(\"{} written!\".format(img_name))\n",
    "\n",
    "            img_counter +=1\n",
    "\n",
    "    cam.release()\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 3)\n",
      "(1, 128, 128, 3)\n",
      "[[ 0.43320486  0.56679505]]\n",
      "1/1 [==============================] - 0s\n",
      "[1]\n",
      "Result : [1] => Positive Hand Gesture\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "C:\\projects\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:325: error: (-215) size.width>0 && size.height>0 in function cv::imshow\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-56c6fade2a93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Hand Gesture is not suitable for tracking the object'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mcapture_Object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGet_Reult_Of_HG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Original/Test/frame_0063.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-56c6fade2a93>\u001b[0m in \u001b[0;36mcapture_Object\u001b[1;34m(output)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Positive Hand Gesture'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mcapture_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Taking image...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-25e616fdd966>\u001b[0m in \u001b[0;36mcapture_object\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: C:\\projects\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:325: error: (-215) size.width>0 && size.height>0 in function cv::imshow\n"
     ]
    }
   ],
   "source": [
    "#Capture image of the object from the camera\n",
    "def capture_Object(output):\n",
    "    \n",
    "    #Checking whether the captured hand gesture is a valid hand gesture to track the object\n",
    "    if output=='Positive Hand Gesture':\n",
    "        \n",
    "        capture_object()\n",
    "                    \n",
    "        print(\"Taking image...\")\n",
    "\n",
    "    else :\n",
    "        print('Hand Gesture is not suitable for tracking the object')\n",
    "        \n",
    "capture_Object(Get_Reult_Of_HG(filepath='Original/Test/hand_gesture_0.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture(0)\n",
    "while( cap.isOpened() ) :\n",
    "    ret,img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray,(5,5),0)\n",
    "    ret,thresh1 = cv2.threshold(blur,70,255,(cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU))\n",
    "  \n",
    "    contours, hierarchy = cv2.findContours(thresh1,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    drawing = np.zeros(img.shape,np.uint8)\n",
    "\n",
    "    max_area=0\n",
    "   \n",
    "    for i in range(len(contours)):\n",
    "            cnt=contours[i]\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if(area>max_area):\n",
    "                max_area=area\n",
    "                ci=i\n",
    "    cnt=contours[ci]\n",
    "    hull = cv2.convexHull(cnt)\n",
    "    moments = cv2.moments(cnt)\n",
    "    if moments['m00']!=0:\n",
    "                cx = int(moments['m10']/moments['m00']) # cx = M10/M00\n",
    "                cy = int(moments['m01']/moments['m00']) # cy = M01/M00\n",
    "              \n",
    "    centr=(cx,cy)       \n",
    "    cv2.circle(img,centr,5,[0,0,255],2)       \n",
    "    cv2.drawContours(drawing,[cnt],0,(0,255,0),2) \n",
    "    cv2.drawContours(drawing,[hull],0,(0,0,255),2) \n",
    "          \n",
    "    cnt = cv2.approxPolyDP(cnt,0.01*cv2.arcLength(cnt,True),True)\n",
    "    hull = cv2.convexHull(cnt,returnPoints = False)\n",
    "    \n",
    "    if(1):\n",
    "               defects = cv2.convexityDefects(cnt,hull)\n",
    "               mind=0\n",
    "               maxd=0\n",
    "               for i in range(defects.shape[0]):\n",
    "                    s,e,f,d = defects[i,0]\n",
    "                    start = tuple(cnt[s][0])\n",
    "                    end = tuple(cnt[e][0])\n",
    "                    far = tuple(cnt[f][0])\n",
    "                    dist = cv2.pointPolygonTest(cnt,centr,True)\n",
    "                    cv2.line(img,start,end,[0,255,0],2)\n",
    "                    \n",
    "                    cv2.circle(img,far,5,[0,0,255],-1)\n",
    "               print(i)\n",
    "               i=0\n",
    "    cv2.imshow('output',drawing)\n",
    "    cv2.imshow('input',img)\n",
    "                \n",
    "    k = cv2.waitKey(10)\n",
    "    if k == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
